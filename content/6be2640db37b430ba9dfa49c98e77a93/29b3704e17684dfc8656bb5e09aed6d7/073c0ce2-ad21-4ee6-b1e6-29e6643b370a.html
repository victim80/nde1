<div class="eg-content-editor" data-type="imageEditorOneColumn"><div class="column">
  <div class="cropped-image" style="position:relative;overflow: hidden;padding-top: 56.13636363636364%">
    <img style="position: absolute; top: 50%;left: 50%;transform: translate(-50%, -50%);max-width: 100%; max-height: 100%;" src="https://files-storage.easygenerator.com/image/357d474a-d593-4ccb-8a80-c3aeb817357e.png" alt="" data-src="https://files-storage.easygenerator.com/image/f0882a2c-1bb6-4199-be9a-01e0fb2710ff.jpg" data-init="{&quot;w&quot;:880,&quot;h&quot;:494.00000000000006,&quot;y&quot;:0,&quot;x&quot;:0,&quot;defaultScale&quot;:1.375,&quot;scale&quot;:0.6875}">
  </div>
  <div class="row" data-content-type="TextEditor"><p style="text-align: justify;">Docker allows connecting multiple containers and services or other non-Docker workloads together. The Docker networking architecture is developed on a set of interfaces known as container network model (CNM). CNM provides application portability across heterogeneous infrastructures. CNM consists of the following five objects:</p><p style="text-align: justify;"><strong>Sandbox</strong>: This contains the configuration of a container’s network stack such as routing table, management of container’s interfaces, and DNS settings. It may have multiple endpoints from various networks. CNM sandbox can be implemented for Windows HNS, Linux network namespace, or a FreeBSD jail.</p><p style="text-align: justify;"><strong>Endpoint</strong>: This connects a sandbox to a network and abstracts the actual connection to the network from the application. An endpoint aids in maintaining portability, to enable the service to utilize various types of network drivers.</p><p style="text-align: justify;"><strong>Network</strong>: A network is a collection of endpoints that have connectivity between them. When a network is created or updated, the corresponding driver is notified. A CNM network can be used to implement a Linux bridge, VLAN, etc.</p><p style="text-align: justify;"><strong>CNM Driver Interfaces</strong>: CNM has two pluggable and open interfaces for the users, vendors, community, etc. to drive additional functionality, visibility, and control in the network. The following are the drivers in the CNM model:</p><ul><li style="text-align:justify;"><strong>Network Drivers</strong>: These are pluggable and provide the actual implementation for the functioning of the network. Multiple network drivers can be simultaneously used on a Docker engine or cluster, but each Docker network is represented by a single driver. There are two types of CNM network drivers.</li><li style="text-align:justify;"><strong><em>Native Network Driver</em></strong><em>: These drivers are provided by Docker.</em></li><li style="text-align:justify;"><em><strong>Remote Network Driver</strong>: These drivers are created by the community and vendors based on their requirements.</em></li><li style="text-align:justify;"><strong>IPAM Drivers</strong>: IP address management (IPAM) drivers in Docker provide default subnets or IP addressing to the network and the endpoints. A user can also assign an IP address manually through the network, container, and service create commands.</li></ul><p style="text-align: justify;"><strong>Docker Native Network Drivers:</strong><strong>&nbsp;</strong>These are native drivers in the Docker engine and can be used through Docker network commands. The following are the various Docker native network drivers.</p><ul><li style="text-align:justify;"><strong>Host</strong>: The host driver enables the container to use the host networking stack.</li><li style="text-align:justify;"><strong>Bridge</strong>: With the bridge driver, a Linux bridge is created on the host, which is managed by the Docker.</li><li style="text-align:justify;"><strong>Overlay</strong>: An overlay network is created with the overlay driver and enables container-to-container communication over the physical network infrastructure.</li><li style="text-align:justify;"><strong>MACVLAN</strong>: With the MACVLAN driver, a network connection is created between container interfaces and its parent host interface (or sub-interfaces).</li><li style="text-align:justify;"><strong>None</strong>: The none driver enables the container to implement its own networking stack and is isolated from the host networking stack.</li></ul></div>
</div></div>